{"version":3,"file":"outputModels.js","sourceRoot":"","sources":["../../src/outputModels.ts"],"names":[],"mappings":"AAAA,uCAAuC;AACvC,kCAAkC","sourcesContent":["// Copyright (c) Microsoft Corporation.\n// Licensed under the MIT license.\n\n/** Represents the outcome of an Image Analysis operation. */\nexport interface ImageAnalysisResultOutput {\n  /** The generated phrase that describes the content of the analyzed image. */\n  captionResult?: CaptionResultOutput;\n  /**\n   * The up to 10 generated phrases, the first describing the content of the whole image,\n   * and the others describing the content of different regions of the image.\n   */\n  denseCaptionsResult?: DenseCaptionsResultOutput;\n  /** Metadata associated with the analyzed image. */\n  metadata: ImageMetadataOutput;\n  /** The cloud AI model used for the analysis */\n  modelVersion: string;\n  /** A list of detected physical objects in the analyzed image, and their location. */\n  objectsResult?: ObjectsResultOutput;\n  /** A list of detected people in the analyzed image, and their location. */\n  peopleResult?: PeopleResultOutput;\n  /** The extracted printed and hand-written text in the analyze image. Also knows as OCR. */\n  readResult?: ReadResultOutput;\n  /**\n   * A list of crop regions at the desired as aspect ratios (if provided) that can be used as image thumbnails.\n   * These regions preserve as much content as possible from the analyzed image, with priority given to detected faces.\n   */\n  smartCropsResult?: SmartCropsResultOutput;\n  /** A list of content tags in the analyzed image. */\n  tagsResult?: TagsResultOutput;\n}\n\n/** Represents a generated phrase that describes the content of the whole image. */\nexport interface CaptionResultOutput {\n  /**\n   * A score, in the range of 0 to 1 (inclusive), representing the confidence that this description is accurate.\n   * Higher values indicating higher confidence.\n   */\n  confidence: number;\n  /** The text of the caption. */\n  text: string;\n}\n\n/**\n * Represents a list of up to 10 image captions for different regions of the image.\n * The first caption always applies to the whole image.\n */\nexport interface DenseCaptionsResultOutput {\n  /** The list of image captions. */\n  values: Array<DenseCaptionOutput>;\n}\n\n/** Represents a generated phrase that describes the content of the whole image or a region in the image */\nexport interface DenseCaptionOutput {\n  /**\n   * A score, in the range of 0 to 1 (inclusive), representing the confidence that this description is accurate.\n   * Higher values indicating higher confidence.\n   */\n  confidence: number;\n  /** The text of the caption. */\n  text: string;\n  /** The image region of which this caption applies. */\n  boundingBox: ImageBoundingBoxOutput;\n}\n\n/** A basic rectangle specifying a sub-region of the image. */\nexport interface ImageBoundingBoxOutput {\n  /** X-coordinate of the top left point of the area, in pixels. */\n  x: number;\n  /** Y-coordinate of the top left point of the area, in pixels. */\n  y: number;\n  /** Width of the area, in pixels. */\n  w: number;\n  /** Height of the area, in pixels. */\n  h: number;\n}\n\n/** Metadata associated with the analyzed image. */\nexport interface ImageMetadataOutput {\n  /** The height of the image in pixels. */\n  height: number;\n  /** The width of the image in pixels. */\n  width: number;\n}\n\n/** Represents a list of physical object detected in an image and their location. */\nexport interface ObjectsResultOutput {\n  /** A list of physical object detected in an image and their location. */\n  values: Array<DetectedObjectOutput>;\n}\n\n/** Represents a physical object detected in an image. */\nexport interface DetectedObjectOutput {\n  /** A rectangular boundary where the object was detected. */\n  boundingBox: ImageBoundingBoxOutput;\n  /** A single-item list containing the object information. */\n  tags: Array<DetectedTagOutput>;\n}\n\n/**\n * A content entity observation in the image. A tag can be a physical object, living being, scenery, or action\n * that appear in the image.\n */\nexport interface DetectedTagOutput {\n  /**\n   * A score, in the range of 0 to 1 (inclusive), representing the confidence that this entity was observed.\n   * Higher values indicating higher confidence.\n   */\n  confidence: number;\n  /** Name of the entity. */\n  name: string;\n}\n\n/** Represents a list of people detected in an image and their location. */\nexport interface PeopleResultOutput {\n  /** A list of people detected in an image and their location. */\n  values: Array<DetectedPersonOutput>;\n}\n\n/** Represents a person detected in an image. */\nexport interface DetectedPersonOutput {\n  /** A rectangular boundary where the person was detected. */\n  readonly boundingBox: ImageBoundingBoxOutput;\n  /**\n   * A score, in the range of 0 to 1 (inclusive), representing the confidence that this detection was accurate.\n   * Higher values indicating higher confidence.\n   */\n  readonly confidence: number;\n}\n\n/** The results of a Read (OCR) operation. */\nexport interface ReadResultOutput {\n  /** A list of text blocks in the image. At the moment only one block is returned, containing all the text detected in the image. */\n  blocks: Array<DetectedTextBlockOutput>;\n}\n\n/** Represents a single block of detected text in the image. */\nexport interface DetectedTextBlockOutput {\n  /** A list of text lines in this block. */\n  lines: Array<DetectedTextLineOutput>;\n}\n\n/** Represents a single line of text in the image. */\nexport interface DetectedTextLineOutput {\n  /** Text content of the detected text line. */\n  text: string;\n  /** A bounding polygon around the text line. At the moment only quadrilaterals are supported (represented by 4 image points). */\n  boundingPolygon: Array<ImagePointOutput>;\n  /** A list of words in this line. */\n  words: Array<DetectedTextWordOutput>;\n}\n\n/** Represents the coordinates of a single pixel in the image. */\nexport interface ImagePointOutput {\n  /** The horizontal x-coordinate of this point, in pixels. Zero values corresponds to the left-most pixels in the image. */\n  x: number;\n  /** The vertical y-coordinate of this point, in pixels. Zero values corresponds to the top-most pixels in the image. */\n  y: number;\n}\n\n/**\n * A word object consisting of a contiguous sequence of characters. For non-space delimited languages,\n * such as Chinese, Japanese, and Korean, each character is represented as its own word.\n */\nexport interface DetectedTextWordOutput {\n  /** Text content of the word. */\n  text: string;\n  /** A bounding polygon around the word. At the moment only quadrilaterals are supported (represented by 4 image points). */\n  boundingPolygon: Array<ImagePointOutput>;\n  /** The level of confidence that the word was detected. Confidence scores span the range of 0.0 to 1.0 (inclusive), with higher values indicating a higher confidence of detection. */\n  confidence: number;\n}\n\n/**\n * Smart cropping result. A list of crop regions at the desired as aspect ratios (if provided) that can be used as image thumbnails.\n * These regions preserve as much content as possible from the analyzed image, with priority given to detected faces.\n */\nexport interface SmartCropsResultOutput {\n  /** A list of crop regions. */\n  values: Array<CropRegionOutput>;\n}\n\n/**\n * A region at the desired aspect ratio that can be used as image thumbnail.\n * The region preserves as much content as possible from the analyzed image, with priority given to detected faces.\n */\nexport interface CropRegionOutput {\n  /**\n   * The aspect ratio of the crop region.\n   * Aspect ratio is calculated by dividing the width of the region in pixels by its height in pixels.\n   * The aspect ratio will be in the range 0.75 to 1.8 (inclusive) if provided by the developer during the analyze call.\n   * Otherwise, it will be in the range 0.5 to 2.0 (inclusive).\n   */\n  aspectRatio: number;\n  /** The bounding box of the region. */\n  boundingBox: ImageBoundingBoxOutput;\n}\n\n/**\n * A list of entities observed in the image. Tags can be physical objects, living being, scenery, or actions\n * that appear in the image.\n */\nexport interface TagsResultOutput {\n  /** A list of tags. */\n  values: Array<DetectedTagOutput>;\n}\n"]}